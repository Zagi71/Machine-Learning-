{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be1b2190-5c71-4484-bfdd-9850b413265f",
   "metadata": {},
   "source": [
    "### Training The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "625724e4-b5f6-4476-85d1-dc86d2ea4e54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dc75c5578154534991945d1f7bd05dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 3.6588 | train_acc: 0.1732 | val_loss: 2.8407 | val_acc: 0.2232 | \n",
      "Epoch: 2 | train_loss: 2.0031 | train_acc: 0.5208 | val_loss: 2.3145 | val_acc: 0.3036 | \n",
      "Epoch: 3 | train_loss: 1.3841 | train_acc: 0.7149 | val_loss: 2.1063 | val_acc: 0.4107 | \n",
      "Epoch: 4 | train_loss: 1.0419 | train_acc: 0.8037 | val_loss: 1.8986 | val_acc: 0.5089 | \n",
      "Epoch: 5 | train_loss: 0.8266 | train_acc: 0.8783 | val_loss: 1.7827 | val_acc: 0.5357 | \n",
      "Epoch: 6 | train_loss: 0.6620 | train_acc: 0.9232 | val_loss: 1.7146 | val_acc: 0.5357 | \n",
      "Epoch: 7 | train_loss: 0.5548 | train_acc: 0.9452 | val_loss: 1.6949 | val_acc: 0.5446 | \n",
      "Epoch: 8 | train_loss: 0.4702 | train_acc: 0.9682 | val_loss: 1.7071 | val_acc: 0.5625 | \n",
      "Epoch: 9 | train_loss: 0.4061 | train_acc: 0.9638 | val_loss: 1.6538 | val_acc: 0.5179 | \n",
      "Epoch: 10 | train_loss: 0.3508 | train_acc: 0.9803 | val_loss: 1.6188 | val_acc: 0.5268 | \n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from tqdm.auto import tqdm\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "# Define constants\n",
    "TRAIN_DIR = \"train/train/\"\n",
    "TEST_DIR = \"test/test/\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "batch_size = 16\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "def set_seeds(seed: int=42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "set_seeds()\n",
    "\n",
    "# Load pre-trained ViT model\n",
    "pretrained_vit_weights = torchvision.models.ViT_B_16_Weights.DEFAULT \n",
    "pretrained_vit = torchvision.models.vit_b_16(weights=pretrained_vit_weights).to(device)\n",
    "\n",
    "# Freeze pre-trained layers\n",
    "for parameter in pretrained_vit.parameters():\n",
    "    parameter.requires_grad = False\n",
    "\n",
    "# Modify head for classification\n",
    "pretrained_vit.heads = nn.Linear(in_features=768,out_features=100).to(device)\n",
    "\n",
    "# Display model summary\n",
    "from torchinfo import summary\n",
    "summary(model=pretrained_vit, \n",
    "        input_size=(batch_size, 3, 224, 224), \n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"])\n",
    "\n",
    "# Define data transformations and create data loaders\n",
    "pretrained_vit_transforms = pretrained_vit_weights.transforms()\n",
    "\n",
    "def create_dataloaders(\n",
    "    train_dir: str, \n",
    "    transform: transforms.Compose, \n",
    "    batch_size: int, \n",
    "    validation_split: float = 0.1,\n",
    "    num_workers: int = os.cpu_count()\n",
    "):\n",
    "\n",
    "    dataset = datasets.ImageFolder(train_dir, transform=transform)\n",
    "\n",
    "    # Split dataset into training and validation sets\n",
    "    num_train = int(len(dataset) * (1 - validation_split))\n",
    "    num_val = len(dataset) - num_train\n",
    "    train_data, val_data = random_split(dataset, [num_train, num_val])\n",
    "\n",
    "    # Create data loaders for training and validation sets\n",
    "    train_dataloader = DataLoader(\n",
    "        train_data,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    val_dataloader = DataLoader(\n",
    "        val_data,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    return train_dataloader, val_dataloader\n",
    "\n",
    "# Create data loaders\n",
    "train_dataloader_pretrained, val_dataloader_pretrained = create_dataloaders(\n",
    "    train_dir=TRAIN_DIR, \n",
    "    transform=pretrained_vit_transforms, \n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "# Define training and evaluation functions\n",
    "def train_step(model: torch.nn.Module, \n",
    "               dataloader: torch.utils.data.DataLoader, \n",
    "               loss_fn: torch.nn.Module, \n",
    "               optimizer: torch.optim.Optimizer,\n",
    "               device: torch.device) -> Tuple[float, float]:\n",
    "    \n",
    "    model.train()\n",
    "    train_loss, train_acc = 0, 0\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(X)\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() \n",
    "        y_pred_class = torch.argmax(y_pred, dim=1)\n",
    "        train_acc += (y_pred_class == y).sum().item()/len(y_pred)\n",
    "    train_loss = train_loss / len(dataloader)\n",
    "    train_acc = train_acc / len(dataloader)\n",
    "    return train_loss, train_acc\n",
    "\n",
    "def test_step(model: torch.nn.Module, \n",
    "              dataloader: torch.utils.data.DataLoader, \n",
    "              loss_fn: torch.nn.Module,\n",
    "              device: torch.device) -> Tuple[float, float]:\n",
    "    model.eval() \n",
    "    test_loss, test_acc = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for batch, (X, y) in enumerate(dataloader):\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            test_pred_logits = model(X)\n",
    "            loss = loss_fn(test_pred_logits, y)\n",
    "            test_loss += loss.item()\n",
    "            test_pred_labels = torch.argmax(test_pred_logits, dim=1)\n",
    "            test_acc += (test_pred_labels == y).sum().item()/len(test_pred_labels)\n",
    "    test_loss = test_loss / len(dataloader)\n",
    "    test_acc = test_acc / len(dataloader)\n",
    "    return test_loss, test_acc\n",
    "\n",
    "def train(model: torch.nn.Module, \n",
    "          train_dataloader: torch.utils.data.DataLoader,\n",
    "          val_dataloader: torch.utils.data.DataLoader,\n",
    "          optimizer: torch.optim.Optimizer,\n",
    "          loss_fn: torch.nn.Module,\n",
    "          epochs: int,\n",
    "          device: torch.device) -> Dict[str, List]:\n",
    "    \n",
    "    results = {\"train_loss\": [],\n",
    "               \"train_acc\": [],\n",
    "               \"val_loss\": [],\n",
    "               \"val_acc\": []}\n",
    "\n",
    "    model.to(device)\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        train_loss, train_acc = train_step(model=model,\n",
    "                                           dataloader=train_dataloader,\n",
    "                                           loss_fn=loss_fn,\n",
    "                                           optimizer=optimizer,\n",
    "                                           device=device)\n",
    "        \n",
    "        val_loss, val_acc = test_step(model=model,\n",
    "                                      dataloader=val_dataloader,\n",
    "                                      loss_fn=loss_fn,\n",
    "                                      device=device)\n",
    "\n",
    "        print(\n",
    "            f\"Epoch: {epoch+1} | \"\n",
    "            f\"train_loss: {train_loss:.4f} | \"\n",
    "            f\"train_acc: {train_acc:.4f} | \"\n",
    "            f\"val_loss: {val_loss:.4f} | \"\n",
    "            f\"val_acc: {val_acc:.4f} | \"\n",
    "        )\n",
    "\n",
    "        results[\"train_loss\"].append(train_loss)\n",
    "        results[\"train_acc\"].append(train_acc)\n",
    "        results[\"val_loss\"].append(val_loss)\n",
    "        results[\"val_acc\"].append(val_acc)\n",
    "\n",
    "    return results\n",
    "\n",
    "optimizer = torch.optim.Adam(params=pretrained_vit.parameters(), lr=learning_rate)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Train the model\n",
    "pretrained_vit_results = train(model=pretrained_vit,\n",
    "                               train_dataloader=train_dataloader_pretrained,\n",
    "                               val_dataloader=val_dataloader_pretrained,\n",
    "                               optimizer=optimizer,\n",
    "                               loss_fn=loss_fn,\n",
    "                               epochs=10,\n",
    "                               device=device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7ef707-d2f3-4276-a7aa-1788addd1772",
   "metadata": {},
   "source": [
    "### Storing the Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6414276e-e6c1-4169-a82c-07b32f3aa24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to save the model weights\n",
    "model_weights_path = \"pretrained_vit_weights.pth\"\n",
    "\n",
    "# Save the trained model weights\n",
    "torch.save(pretrained_vit.state_dict(), model_weights_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758d8c2f-ebfe-4050-b34f-cf33d9c96aa1",
   "metadata": {},
   "source": [
    "### Sample Code to for prediction using the stored, Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44ad0e81-9651-465b-8c59-afb4659c161b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is a map, between the labels, I used for different classes and the actuall class names in the train/train directoryu\n",
    "my_dict = {\n",
    "    0: 0,\n",
    "    1: 1,\n",
    "    10: 2,\n",
    "    11: 3,\n",
    "    12: 4,\n",
    "    13: 5,\n",
    "    14: 6,\n",
    "    15: 7,\n",
    "    16: 8,\n",
    "    17: 9,\n",
    "    18: 10,\n",
    "    19: 11,\n",
    "    2: 12,\n",
    "    20: 13,\n",
    "    21: 14,\n",
    "    22: 15,\n",
    "    23: 16,\n",
    "    24: 17,\n",
    "    25: 18,\n",
    "    26: 19,\n",
    "    27: 20,\n",
    "    28: 21,\n",
    "    29: 22,\n",
    "    3: 23,\n",
    "    30: 24,\n",
    "    31: 25,\n",
    "    32: 26,\n",
    "    33: 27,\n",
    "    34: 28,\n",
    "    35: 29,\n",
    "    36: 30,\n",
    "    37: 31,\n",
    "    38: 32,\n",
    "    39: 33,\n",
    "    4: 34,\n",
    "    40: 35,\n",
    "    41: 36,\n",
    "    42: 37,\n",
    "    43: 38,\n",
    "    44: 39,\n",
    "    45: 40,\n",
    "    46: 41,\n",
    "    47: 42,\n",
    "    48: 43,\n",
    "    49: 44,\n",
    "    5: 45,\n",
    "    50: 46,\n",
    "    51: 47,\n",
    "    52: 48,\n",
    "    53: 49,\n",
    "    54: 50,\n",
    "    55: 51,\n",
    "    56: 52,\n",
    "    57: 53,\n",
    "    58: 54,\n",
    "    59: 55,\n",
    "    6: 56,\n",
    "    60: 57,\n",
    "    61: 58,\n",
    "    62: 59,\n",
    "    63: 60,\n",
    "    64: 61,\n",
    "    65: 62,\n",
    "    66: 63,\n",
    "    67: 64,\n",
    "    68: 65,\n",
    "    69: 66,\n",
    "    7: 67,\n",
    "    70: 68,\n",
    "    71: 69,\n",
    "    72: 70,\n",
    "    73: 71,\n",
    "    74: 72,\n",
    "    75: 73,\n",
    "    76: 74,\n",
    "    77: 75,\n",
    "    78: 76,\n",
    "    79: 77,\n",
    "    8: 78,\n",
    "    80: 79,\n",
    "    81: 80,\n",
    "    82: 81,\n",
    "    83: 82,\n",
    "    84: 83,\n",
    "    85: 84,\n",
    "    86: 85,\n",
    "    87: 86,\n",
    "    88: 87,\n",
    "    89: 88,\n",
    "    9: 89,\n",
    "    90: 90,\n",
    "    91: 91,\n",
    "    92: 92,\n",
    "    93: 93,\n",
    "    94: 94,\n",
    "    95: 95,\n",
    "    96: 96,\n",
    "    97: 97,\n",
    "    98: 98,\n",
    "    99: 99\n",
    "}\n",
    "\n",
    "# Exchange keys and values using dictionary comprehension\n",
    "exchanged_dict = {v: k for k, v in my_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ade7611-96ec-4208-b6d6-2ef4bdb4773a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "#Replace this with the actuall path\n",
    "model_weights_path = \"pretrained_vit_weights.pth\"\n",
    "test_image_path = \"test\\\\test\\\\0.jpg\"\n",
    "def preprocess_image_error(image_path):\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    image = transform(image).unsqueeze(0)  # Add batch dimension\n",
    "    return image.to(device)\n",
    "def preprocess_image(image_path):\n",
    "    try:\n",
    "        transform = pretrained_vit_transforms\n",
    "        image = Image.open(image_path)\n",
    "        image = transform(image).unsqueeze(0)  # Add batch dimension\n",
    "        return image.to(device)\n",
    "    except:\n",
    "        return preprocess_image_error(image_path)\n",
    "\n",
    "def predict_image_class(image_path, model):\n",
    "    image = preprocess_image(image_path)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(image)\n",
    "    probabilities = torch.softmax(output, dim=1)[0]\n",
    "    predicted_class_index = torch.argmax(probabilities).item()\n",
    "    return predicted_class_index, probabilities\n",
    "\n",
    "model = pretrained_vit\n",
    "\n",
    "\n",
    "model.load_state_dict(torch.load(model_weights_path))\n",
    "\n",
    "\n",
    "predicted_class_index_map, probabilities = predict_image_class(test_image_path, model)\n",
    "\n",
    "predicted_class_index = exchanged_dict[predicted_class_index_map]\n",
    "print(\"Predicted Class:\", predicted_class_index)\n",
    "print(\"Probabilities:\", probabilities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a22174-7ffe-4903-9ba5-ecbc868325f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
